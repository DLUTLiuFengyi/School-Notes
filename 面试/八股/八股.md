---
typora-root-url: pic
---

### 高并发 高性能 高可用

#### 系统拆分

一个拆成多个，每个连接一个DB，这样就多个DB了

#### 缓存

必须。大部分高并发都是读多写少，完全可以在DB和缓存都写一份，读的时候走缓存。

redis单机几万并发。

#### 消息队列

必须。因为还是会出现高并发写。

大量的写请求灌入MQ，排队慢慢玩（排队等待），后边系统消费后慢慢写。

可以考虑异步写提高并发性。

#### 分库分表

DB层面。一个库拆成多个库来抗高并发，一个表拆成多个表提高跑sql性能。

#### 读写分离

大部分DB都是读多写少，没必要所有请求都集中在一个库上。搞个主从架构，主库写入从库读取。

#### ES

### Kafka

kafka其实有三个功能：消息队列，持久式存储，流式处理

#### 优势

大量使用批量处理和异步思想，最高每秒千万级

基于scala和java编写，大数据和流式处理生态兼容良好

#### 模型

##### 队列模型

队列作为消息通信载体，生产-消费者模式

缺点是不适用于：将生产者产生的消息分发给多个消费者且每个消费者都能接收到完整的消息内容

##### 发布-订阅模型

为了解决队列模型的问题

使用topic作为消息通信载体，类似广播，发布者发布一条消息，该消息通过topic传递给所有订阅者

队列模型是此模型的特例（只有一个订阅者）

#### 概念

producer生产者，consumer消费者，broker代理：独立的kafka实例，多个kafka broker组成一个kafka集群

partition分区：属于topic的一部分，一个topic有多个partition，同一topic下的partition可以分布在不同的broker，实际对应于消息队列里的队列

#### 多副本机制

为分区引入多副本机制，多副本间有leader，其他副本称为follower，我们发送的消息会被发送到leader，然后follower才能从leader中拉取消息进行同步

##### 好处

* 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）
* Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力

#### zookeeper

主要为kafka提供元数据管理

* broker注册：有一个专门**用来进行 Broker 服务器列表记录**的节点，每个 Broker 在启动时，都会到 Zookeeper 上进行注册，即到/brokers/ids 下创建属于自己的节点。每个 Broker 就会将自己的 IP 地址和端口等信息记录到该节点中去

* topic注册：同一个**Topic 的消息会被分成多个分区**并将其分布在多个 Broker 上，**这些分区信息及与 Broker 的对应关系**也都是由 Zookeeper 在维护

* 负载均衡： 对于同一个 Topic 的不同 Partition，Kafka 会尽力将这些 Partition 分布到不同的 Broker 服务器上。当生产者产生消息后也会尽量投递到不同 Broker 的 Partition 里面。当 Consumer 消费的时候，Zookeeper 可以根据当前的 Partition 数量以及 Consumer 数量来实现动态负载均衡。

#### 如何保证消息消费顺序

每次添加消息到 Partition的时候都会采用尾加法

Kafka 只保证 Partition中的消息有序，而不能保证 Topic中的 Partition 的有序。

因此，如何保证 Kafka 中消息消费的顺序有下面两种方法：

1. 1 个 Topic 只对应一个 Partition。
2. （推荐）发送消息的时候指定 key/Partition。

消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。

#### 如何保证消息不丢失

##### 生产者丢失消息

检查失败原因，重新发送，可以设置重试次数、重试间隔

##### 消费者丢失消息

消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。

当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。

**手动关闭自动提交 offset，每次在真正消费完消息之后之后再自己手动提交 offset 。** 但是，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。

##### kafka丢失消息

### 实战

#### 服务限流

#### 100亿个数topk

### Mybatis

MyBatis 是一个支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以**使用简单的 XML 或注解来配置和映射原生信息，将接口和普通Java对象映射成数据库中的记录**。

Mybatis的运行分为两个部分，第一部分是读取配置文件缓存到Configuration对象，用以创建SqlSessionFactory，第二部分是SqlSession的执行过程。

流程
1、使用XML配置文件或Java代码方式生产SqlSessionFactory

2、使用Resources类的getResourceAsStream()方法读取XML配置文件

3、使用SqlSessionFactoryBuilder类的build()方法创建sqlSessionFactory

4、得到sqlSessionFactory类后使用该类的openSession()获取SqlSession

5、得到SqlSession后需要实现映射器的功能，映射器有一个接口和该接口对应的XML映射文件或使用注解组成

6、使用SqlSession的getMapper()方法得到具体的接口类对象

步骤
1、创建MyBatis配置文件：mybatis-config.xml（不使用Java代码方式）（1）配置数据库环境（2）配置映射器的XML映射文件2、创建一个生成SqlSession的sqlSessionFactory类：SqlSessionFactoryUtils。对应流程的2、3、4

3、定义一个接口和该接口对应的XML映射文件：XxxMapper、XxxMapper.xml（不使用注解方式）

4、使用SqlSession的getMapper()方法得到具体的接口类对象

5、执行完业务后记得关闭sqlSession

#### 如何对jdbc封装

SqlSession接口通过DefualtSqlSession实例调用Executor接口实现对sql语句的处理和封装

* MyBatis核心配置文件：配置数据源、事务管理方式、指定sql映射文件位置
* SqlSessionFactory：根据核心配置文件生产的工厂对象，作用是创建SqlSession
* SqlSession：提供开发人员的一个接口，作用是操作数据库增删改查
* Executor：一个在SqlSession内部使用的接口，作用是负责对数据库的具体操作
* mapped statement：底层封装工具，作用是生成具体的sql命令以及对查询结果集二次封装

#### dirty属性

SqlSession的成员属性，变成true时表示即将改DB的数据，代表数据库数据与内存中数据不一致了，需要更新
