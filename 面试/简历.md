---
typora-root-url: pic
---

### Spark

#### Driver和Executor

它们是计算相关组件

Driver是入口，负责向集群申请资源，切分与调度，dagmanager，taskmanager分发代码到worker节点

Executor有2个核心功能

* 运行组成spark应用的任务，并将结果返回给驱动器进程

* 通过自身的Block Manager为用户程序中要求缓存的RDD提供 **内存式存储** 

  RDD是直接缓存在exeutor进程中的，因此任务可以在运行时充分利用缓存数据加速运算

Executor是集群运行在工作节点（worker）中的一个jvm进程，是集群中专门计算的节点，主要执行task

#### Master和Worker

它们是资源相关组件

只在standalone模式存在这两个概念，因为需要spark自己提供资源管理和资源调度。

类似对比于yarn中的ResourceManager和NodeManager

master：一个进程，主要负责资源调度和分配，并进行集群的监控等

worker：也是进程，一个worker运行在集群中的一台主机上，由master分配资源对数据进行并行处理和计算

worker是包工头，负责管理其节点上的executor

#### ApplicationMaster

如果计算和资源（Driver和Master）直接进行交互，则会增加耦合度

因此让D和M并不能直接交互

在二者间放一个ApplicationMaster

Driver委托AM，AM再向Master申请资源

#### Task

task分为resultTask和shuffleMapTask

#### Shuffle

需要shuffle：某种具有共同特征的数据汇聚到一个计算节点进行计算

#### 分区

每个数据分片都对应具体物理位置，数据的位置是被blockManager管理

#### 序列化

减少存储空间 缺点是使用时需要反序列化，很耗cpu

将不能被序列化的对象封装成object

一个app对应一个context，app存在多个job，一个action触发一个job

#### task快速启动

executor生命周期与app一样，即使没有job运行也存在，所以task可以快速启动

#### 为什么比mapreduce快

* 基于内存计算，减少低效磁盘交互
* 高效调度算法，基于DAG
* 容错机制 血缘关系 

#### DAGScheduler

##### stage划分算法说明

从触发action操作的rdd开始往前倒推，首先会为最后一个rdd创建一个stage,继续往前倒退的时候，如果发现对某个 rdd是宽依赖，那么就会将该宽依赖的rdd创建一个新的stage,之前面的那个rdd就是新的stage的最后一个rdd。然后以次类推，继续往前倒退，根据窄依赖和宽依赖进行stage的划分，知道所有的rdd全部遍历完成。

##### 划分stage的作用

在spark中提交的应用都会以**job**的形式进行执行，job提交后会被划分为多个**stage**,然后把stage封装为**TaskSet**提交到**TaskScheduler**到executor中执行。

#### 与mapreduce区别

* hadoop一个app是一个job，spark一个app有多个job

  hadoop的多个job需要自己管理关系

* hadoop在mr过程中会重复读写hdfs，造成大量io

* spark的迭代计算在内存中进行

* spark的DAG可以实现良好容错性（RDD不可变）

### Flink

#### 与Spark Streaming区别

#### 增量迭代

### Kubernetes

Kubernetes是用于自动部署，扩展和管理容器化应用程序的开源系统。它将组成应用程序的容器组合成逻辑单元，以便于管理和服务发现。

#### 特性

1. **服务发现与负载均衡**：无需修改你的应用程序即可使用陌生的服务发现机制。
2. 存储编排：自动挂载所选存储系统，包括本地存储。
3. Secret和配置管理：部署更新Secrets和应用程序的配置时不必重新构建容器镜像，且不必将软件堆栈配置中的秘密信息暴露出来。
4. 批量执行：除了服务之外，Kubernetes还可以管理你的批处理和CI工作负载，在期望时替换掉失效的容器。
5. **水平扩缩**：使用一个简单的命令、一个UI或基于CPU使用情况自动对应用程序进行扩缩。
6. 自动化上线和回滚：Kubernetes会分步骤地将针对应用或其配置的更改上线，同时监视应用程序运行状况**以确保你不会同时终止所有实例**。
7. 自动装箱：根据资源需求和其他约束自动放置容器，同时避免影响可用性。
8. **自我修复**：重新启动失败的容器，在节点死亡时替换并重新调度容器，杀死不响应用户定义的健康检查的容器。

#### 结构

* Master：管理整个集群，协调所有活动（调度、维护、扩容、更新）

* Node：托管正在运行的应用，可以是一个虚拟机或物理机，充当工作机器角色

* Deployment：负责创建和更新应用程序的实例

* Pod：逻辑主机，负责托管应用实例，包括多个应用程序容器及这些容器的共享资源（存储、网络、运行信息）

  一个Node有多个Pod，一个Pod有多个容器

* Service：抽象层，定义了一组Pod的逻辑集合（因此可以跨Node组成一个Service），为这些pod支持外部流暴露、负载均衡和服务发现

  尽管每个Pod 都有一个唯一的IP地址，但是如果没有Service，这些IP不会暴露在群集外部。Service允许您的应用程序接收流量。

<img src="/k8s-service.webp" style="zoom:50%;" />

#### 项目为什么用k8s

#### 组件

* Master组件

  对集群进行全局决策，检测和响应集群时间（如副本数减少时自动创建新pod）

  * kube-apiserver：master节点上提供k8s api服务的组件

  * etcd：保存了k8s集群的一些数据，如pod的副本数，pod的期望状态与现在状态

  * scheduler：master节点上的调度器，负责选择节点让pod在节点上运行

    kube-scheduler 给一个 pod 做调度选择包含两个步骤：
     1、过滤
     2、打分

    过滤阶段会将所有满足 Pod 调度需求的 Node 选出来。例如，PodFitsResources 过滤函数会检查候选 Node 的可用资源能否满足 Pod 的资源请求。在过滤之后，得出一个 Node 列表，里面包含了所有可调度节点；通常情况下，这个 Node 列表包含不止一个 Node。如果这个列表是空的，代表这个 Pod 不可调度。

    在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node。根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。

    最后，kube-scheduler 会将 Pod 调度到得分最高的 Node 上。如果存在多个得分最高的 Node，kube-scheduler 会从中随机选取一个。

  * controller：master节点的控制器，在节点出现故障时进行通知和响应，对节点的pod状态进行监控

* Node组件

  * kubelet：管理Node而且是Node与Master通信的代理，保证容器都运行在pod中
  * kube-proxy：一个代理，通过代理创建一个虚拟ip，**通过这个ip与pod进行交流**
  * container runtime：容器运行环境，负责在节点上运行容器的软件

* 附加组件

  * DNS：对k8s集群进行域名解析
  * Dashboard
  * 集群层面日志：负责将容器的日志数据保存到一个集中的日志存储中，该存储能提供搜索和浏览接口
  * 容器资源监控：将关于容器的常见时间序列度量值保存到一个集中的数据库中，提供界面

#### k8s流程

1. 准备好yaml，通过kubectl发送到api server中
2. api server接收到客户端的请求，将请求内容保存到etcd中
3. scheduler监测etcd，发现没有分配节点的pod对象，通过过滤和打分筛选出最适合的节点运行pod
4. 节点通过container runtime运行对应pod的容器以及创建对应的副本数
5. 节点上的kubelet会对自己节点上的容器进行管理
6. controler会监测集群中的每个节点，发现期望状态和实际状态不符合的话，通知对应的节点
7. 节点收到通知，通过container runtime来对pod内的容器进行收缩和扩张

对于容器来说，管理的单位是容器
对于k8s来说，管理的是一个pod应用
一个pod上可以运行多个容器，可以将pod理解为一个虚拟机，一个虚拟机上运行了多个容器

#### 通信

#### 弹性伸缩机制

在 Kubernetes 的生态中，在多个维度、多个层次提供了不同的组件来满足不同的伸缩场景。

有三种弹性伸缩

- CA（Cluster Autoscaler）：Node级别自动扩/缩容cluster-autoscaler组件
- HPA（Horizontal Pod Autoscaler）：Pod个数自动扩/缩容
- VPA（Vertical Pod Autoscaler）：主要使用场景是有状态应用

##### CPA

扩容：Cluster AutoScaler定期检测是否有充足的资源来调度新创建的pod，当资源不足时会调用Cloud Provider创建新的Node

缩容：Cluster AuthScaler也会定期检测Node的资源使用情况，当一个Node长时间资源利用率都很低时自动将其所在虚拟机从云服务商中删除。此时，原来的Pod会自动调度到其他Node上面。

##### HPA

**定义**

根据资源利用率或者自定义指标自动调整replication controller, replica set或deployment，实现部署的自动扩展和缩减。

操作对象是Replication Controller、ReplicaSet或者Deployment对应的Pod（k8s中可以控制Pod的是rc、rs、deployment），根据观察到的**CPU使用量与用户的阈值**进行比对，做出是否需要增加或者减少**实例数量**的决策。controller目前使用heapSter来检测CPU使用量，检测周期默认是30秒。

**工作原理**

Metrics Server 持续采集所有 Pod 副本的指标数据(CPU使用率)。HPA 控制器通过 Metrics Server 的 API获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标 Pod **副本数量**。当目标 Pod 副本数量与当前副本数量不同时，HPA 控制器就向 **Pod 的副本控制器（Deployment、RC 或 ReplicaSet）**发起 scale 操作，调整 Pod 的副本数量，完成扩缩容操作 

**冷却周期**

为了保证集群的稳定性。由于评估的度量标准是动态特性，副本的数量可能会不断波动。有时被称为颠簸， 所以在每次做出扩容缩容后，冷却时间是多少。在 HPA 中，默认的扩容冷却周期是 3 分钟，缩容冷却周期是 5 分钟。

#### Deployment

负责创建与更新应用程序的实例，位于Master中。创建Deployment后，Kubernetes Master 将**应用程序**实例**调度**到集群中的各个**节点**上。如果托管实例的**节点**关闭或被删除，Deployment控制器会将该实例替换为群集中**另一个节点**上的实例。这提供了一种自我修复机制来解决机器故障维护问题。

可以使用Kubernetes命令行界面Kubectl创建和管理Deployment。Kubectl使用Kubernetes API与集群进行交互。

#### exec

#### 如何支持有状态应用

### Docker

docker容器实际上就是运行的一个进程，只不过由于docker帮助我们包装了这个进程，给这个进程加以一个可运行的微linux环境而已，让我们感觉看起来"像"虚拟机而已。所以也就不奇怪，为什么容器的启动是秒级的，启动一个虚拟机是分钟级别的。

#### 与虚拟机区别

docker提供的是linux内核中最核心的服务，基于namespace提供基本隔离，基于cgroup提供资源隔离，启动开销是秒级

docker虚拟内存地址 -> 宿主机物理内存地址

虚拟机额外有个hypervision进行对物理宿主机的整个抽象，启动开销是分钟级，因为要先启动一个完整的虚拟操作系统

vm虚拟内存地址 -> 抽象物理内存地址 -> 宿主机物理内存地址

