## TaskExecutor内存模型

<img src="D:\ideaprojects\School-Notes\Flink\pic\m1.png" style="zoom:50%;" />

process memory是一个task executor进程使用的所有内存

向yarn或k8s申请的每一个容器都有一个资源的限定，如果超过限定，可能会被yarn和k8s删掉

flink memory是把jvm自身开销刨除了

### 内存种类

#### Framework vs Task Memory

<img src="D:\ideaprojects\School-Notes\Flink\pic\m2.png" style="zoom:50%;" />

区别：内存是否计入slot资源

```txt
slot是执行作业的最小单位，是一个executor的资源的子集
一个TaskExecutor是一个进程，内部slot和framework都是线程
```

哪怕一个task没有分配，都得给框架留一点内存空间（framework heap/off-heap）

二者的总用量受限

* -Xmx = Framework Heap + Task Heap
* -XX:MaxDirectMemorySize = ...+Framework Off-Heap + Task Off-Heap

无隔离

内存一般不会为线程级别作区别，引入framework与task的区别是为了为后续版本作准备

现在slot是内存的等价划分（三分之一...）

**动态切割slot资源**：FLIP-56 Dynamic Slot Allocation

动态切割出不同大小的slot，而又留一部分给framework使用

现在建议使用默认framework值

https://cwiki.apache.org/confluence/display/FLINK/FLIP-56%3A+Dynamic+Slot+Allocation

#### Heap & Off-Heap Memory

Heap

* 绝大多数java对象
* HeapStateBackend

Off-Heap

* Direct

  * DirectByteBuffer
    * ByteBuffer.allocateDirect()
  * MappedByteBuffer
    * FileChannel.map()

* Native

  不受jvm管控的内存

  * JNI, C/C++, Python, ...

* flink不要求用户对direct/native进行区分

  基于假设：在绝大多数场景中很少有人用

#### Network Memory

属于Direct Memory

在设置了指定大小后，在集群一启动后，会把所有配置的大小都申请下来，并且在executor shutdown之前不会释放

用于数据传输buffer

* 同一TaskExecutor的各slot之间没有隔离
* 一个task需要多少N M由task的拓扑（physical graph）决定，不足会导致运行失败，多了在flink目前版本来说也用不上

task的网络内存计算

```java
networkMemory = bufferSize<默认是32KB>*(inputBuffers+outputBuffers)
inputBuffers = remoteChannels*buffersPerChannel<2> + gates*buffersPerGates<8>
outputBuffers = subpartitions + 1
    
channel - physical graph中需要进行网络通信的边的数量
remoteChannel - 不在当前TM的上游Subtask(physical graph vertex)数量
gates - 上游Task(logical graph vertex)数量
subpartitions - 下游Subtask(physical graph vertex)数量
```

#### Managed Memory

不受到jvm限制，不完全受jvm掌控

native memory

用于

* RocksDBStateBackend

* Batch Operator

  HeapStateBackend/无状态 不需要Managed Memory，可设为0避免浪费

特点

* 同一taskExecutor的各slot之间严格隔离
* 多点少点都能跑，与性能挂钩

RocksDB内存限制

* state.backend.rockdb.memory.managed (true)
* 设置RocksDB使用的内存大小为Managed Memory大小
* 目的是防止容器内存超用

standalone模式下可以关闭限制以避免影响RocksDB性能

#### JVM Metaspace & Overhead

jvm自身开销

jvm metaspace

* 存放jvm加载的类的元数据
* 加载的类越多，需要的空间越大

以下情况需要注意调大jvm metaspace

* 作业需要加载大量第三方库
* 多个不同作业的任务运行在同一个taskExecutor上

jvm overhead

* native memory
* 用于其他jvm开销
  * code cache
  * thread stack

### 内存类型解读

java内存说到底只有两种：heap与off-heap

**heap**

经过jvm简化，java对象不用分析这个对象存放在内存哪里

* 经过jvm虚拟化的内存
* 实际存储位置可能随gc变化，上层app无感

java对象会在堆内存中频繁copy

**off-heap**

* 未经jvm虚拟机的内存，直接由os决定
* 直接映射到本地os内存地址

避免了频繁gc的拷贝过程 os外设、网络写出、文件写出避免在应用层与内核空间之间进行复制的成本 效率更高

<img src="D:\ideaprojects\School-Notes\Flink\pic\m3.png" style="zoom:50%;" />

#### Heap Memory 特性

内存并不是app释放时就释放，而是在达到内存使用上限后释放

用量上限受jvm严格控制

* -Xmx

  Framework Heap + Task Heap

* 达到上限后触发gc
* gc后空间仍然不足，触发oom异常并退出 outofmemoryerror java heap space

#### Direct Memory & Metaspace特性

**Direct M**

* Framework off-heap Memory（部分）
* Task off-heap Memory（部分）
* Network Memory

用量上限受jvm严格控制

* -XX:MaxDirectMemorySize

  Framework Heap + Task Heap + Task off-Heap + Network Memory

* 达到上限后触发gc
* gc后空间仍然不足，触发oom异常并退出 outofmemoryerror: direct buffer memory

**Metaspace**

用量上限受jvm严格控制

* -XX:Metaspacesize
* 达到上限时触发gc，gc后仍然空间不足触发oom异常并退出 outofmemoryerror: metaspace

为什么不是堆上内存，还需要jvm的gc管理？因为都是在jvm中有相应的java对象与其对应，类的加载生命周期结束 java对象没用了，但只有在gc时才会真正对象释放，也只有gc时对象对应的堆外内存才会释放，所以也是需要依赖gc的。

#### Native Memory特性

最大特点：完全不受jvm控制

* Framework off-heap（部分）
* Task off-heap（部分）
* Managed Memory（flink自己掌管，不由jvm管）
* jvm overhead

用量上限不受jvm严格控制

#### Framework / Task off-heap Memory

<img src="D:\ideaprojects\School-Notes\Flink\pic\m5.png" style="zoom:60%;" />

### 配置

<img src="D:\ideaprojects\School-Notes\Flink\pic\m6.png" style="zoom:60%;" />



## Flink数据类型和序列化

TypeInformation

通过自定义TypeInfo为任意类提供flink原生内存管理（而非kryo）可令存储紧凑，运行时更高效。需要注意在自定义类上使用@TypeInfo注解，随后创建相应的TypeInfoFactory并覆盖createTypeInfo()方法

```java
@TypeInfo(MyTupleInfoFactory.class)
public class MyTuple<T0, T1> {
    public T0 myfield0;
    public T1 myfield1;
}

public class MyTupleInfoFactory extends TypeInfoFactory<MyTuple> {
    @Override
    public TypeInformation<MyTuple> createTypeInfo(Type t, Map<String,TypeInformation<?>> genericParameters) {
        return new MyTupleTypeInfo(genericParameters.get("T0"),genericParameters.get("T1"));
    }
}
```

